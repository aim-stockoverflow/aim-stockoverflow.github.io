import React, { Component, PropTypes } from 'react';
import { Card,Col,Row } from 'antd';

export default class Team extends Component {
  render() {
    const TechData = [
      {
        headline: '引言',
        content: <p>近年来，由于大数据时代的来临，数据挖掘、机器学习等领域迎来了一次巨大的飞跃性发展。而alphaGo的横空出世，则真正意义上吸引了大众的目光，再一次提高了机器学习与人工智能领域的热度。而提到机器学习，就不得不提到增强学习算法。行业领头团队DeepMind和风头正劲的后起之秀OpenAI不约而同的把大量的精力放在了这一领域，互联网巨头Google也毫不掩饰对其的重视与期待。本项目组利用增强学习与神经网络结合成的DQN（Deep Q-Network）来作为交易策略的投资依据，而这篇文章就将向大家解释清楚我们使用的算法背后的原理。</p>
      },
      {
        headline: '增强学习算法',
        content: (
          <div>
            <p>什么是增强学习算法？Andrew Ng曾经对此做出过精彩的比喻：“假想你在教你的狗狗叼回被你扔出去的飞盘，当它做到目标的时候，你会摸摸它的脑袋并对它说：'Good dog'；否则你或许会拍他一下并对它说：'Bad dog'。那么情况转移到这台（使用增强学习算法训练的无人）直升机，当它能很好的飞行时，你就对它说：'Good helicopter'；否则你就会说：'Bad helicopter'。”</p>
            <p>这当然是玩笑之语，不过却能很贴切的讲明增强学习的原理：利用奖惩机制来对一个一个执行操作的机器（我们称之为agent）进行训练，当它做到了我们期望的事情，我们就给予其奖赏；而因此它就会调整自己的运动策略，以便得到更多的奖励。</p>
            <br />
            <ol>
              <h4>增强学习的实现需满足三个条件：</h4>
              <li>解决问题的环境需满足马尔科夫决策过程（Markov Decision Process，MDP）</li>
              <li>问题解决过程中的每一步动作可以用奖赏（reward）机制来描述</li>
              <li>Agent在反复尝试中根据每一步reward的值来改变自身参数从而实现增强学习</li>
            </ol>
            <br />
            <h4>马尔科夫决策过程（Markov Decision Process，MDP）</h4>
            <p>MDP中，用（S，A，P）表示状态，动作，转移概率， St+1与（St，At，P）存在对应关系，即当前时刻的所有状态决定了下一时刻环境的状态。</p>
            <br />
            <h4>奖赏（reward）机制</h4>
            <p>Reward机制中，Value（S）函数使得每一个S对应一个Value值，每一步决策完成后，根据决策结果S的value值，agent将会获取一个reward。当agent做了决策之后，它能得到的reward的准确度至关重要。准确度将保证agent针对一个状态做出这个状态下最优化的决策。对于reward的获取，可以采用的方法有：值迭代，策略迭代。当经过多次迭代以后，reward将会收敛，其准确度将满足需求。</p>
            <p>由于问题解决的环境往往相当复杂，Agent的策略函数也会变得异常复杂，如此复杂的函数几乎不可能用固定的公式来表达，因此策略函数一般只能用神经网络来拟合。</p>
          </div>
        )
      },
      {
        headline: '神经网络',
        content: (
          <div>
            <p>神经网络由于可以拟合很复杂的函数，为找出获得（S，A）与reward的对应关系提供了可行的解决方法。神经网络函数f可以实现f(Si，Aj)=Rij。（而这是为了便于理解做出的描述，实际上我们是用神经网络来完成从状态S到S下可执行的各个动作的价值，但是原理是一致的）。S与A作为函数f的自变量X，而f的参数为W（X与W都是向量），同时有f=WX。因此，找到正确的（或是可以接受的）W即可解决问题。</p>
            <p>找到可接受的W值可以采用梯度下降的方式。梯度下降法，即用来找函数的最小值的方法。在函数上某一个点计算函数的梯度，然后向梯度方向移动一小步，从而沿着梯度下降的方向求解直至获取极小值。</p>
            <p>对于选择哪个函数来梯度下降，可以利用极大似然估计法来获得似然函数，进而对W进行估计，最终得到所谓的“代价函数”：</p>
            <p className="img"><img src={require('images/techIntro/1.1.png')} /></p>
            <p>关于各项参数的实际意义，技术分享中中将有更详细的说明。由于W可以计算，利用神经网络经过一定的训练后就可以形成状态空间S到动作空间A的映射。agent就可以输入状态S的各项参数，反馈最优化的动作A。</p>
            <p>采用神经网络来实现增强学习的Agent的策略函数，便是DQN，同时也是我们项目中所采用的核心技术。</p>
          </div>
        )
      },
      {
        headline: '股票（股指）与DQN的结合',
        content: (
          <div>
              <p>要完成结合，只需要满足前文提到的三个条件即可。首先，当前的状态一定可以决定未来的状态吗？我们认为，当将整个市场的状态尽收眼底的时候，就可以认为我们可以根据这一状态推算出第二天的情报。但是显然，由于器材限制与信息的难以获得，我们很难获得上帝视角。因此我们只能尽量的挑选出其中最为重要的几个参数来代表当前的状态。或许结果并不是最好的，但是只要得到的结果可以接受，这一条件就可以认为已经被满足了。</p>
              <p>其次，如何获得状态或是动作的价值呢？在长时间的学习与试验中，笔者发现投资市场与其他的情况是有一定区别的。可以认为，在一定程度上，一次投资的价值是完全可以直接被计算出来的。我们认为可以分两部分来看：首先，必须要体现agent对状态的预测的奖赏，比如说如果它在股票涨的时候打算执行买入操作，我们就会奖赏它，而且涨得越多买入越多奖赏就应该越多；如果在股票跌的时候打算执行卖出操作，同样我们也应该奖赏它，而且跌的越多卖出越多奖赏也应该越多。其次，必须要体现对盈利的奖赏。这就是显而易见的了——如果股票大跌而agent取出了百分之十的资金，那么剩余百分之九十的资金仍然在产生损失，这也是我们需要考虑的。</p>
              <p>最后就是神经网络的结合。在训练神经网络的过程中，由于小资金的投资与其他情况不同，有着“不同的动作难以对大环境产生影响”这一特性，我们可以对训练进行一定的简化，即只训练可接受的动作，对不可训练的动作产生的数据进行丢弃。这样，我们一方面提高了训练的速度，一方面也让训练更加的可靠。</p>
          </div>
        )
      },
      {
        headline: '其他支持',
        content: (
          <div>
              <p>为了更快的完成项目，同时为了提高程序的效率与可靠性，我们利用开源项目作为基础进行开发。神经网络的搭建我们使用了开源项目TensorFlow。TensorFlow是谷歌的第二代机器学习系统，在某些基准测试中，TensorFlow的表现比第一代的DistBelief快了2倍。TensorFlow 内建深度学习的扩展支持，任何能够用计算流图形来表达的计算，都可以使用TensorFlow。任何基于梯度的机器学习算法都能够受益于TensorFlow的自动分化（auto-differentiation）。通过灵活的Python接口，要在TensorFlow中表达想法也会很容易。</p> 
              <p>而我们做测试时使用的股票数据则是来源于另一个开源项目TuShare。 TuShare是一个免费、开源的python财经数据接口包。主要实现对股票等金融数据从数据采集、清洗加工到数据存储的过程，能够为金融分析人员提供快速、整洁、和多样的便于分析的数据，为他们在数据获取方面极大地减轻工作量，使他们更加专注于策略和模型的研究与实现上。考虑到Python pandas包在金融量化分析中体现出的优势，TuShare返回的绝大部分的数据格式都是pandas DataFrame类型，非常便于用NumPy进行数据分析和可视化。</p> 
              <p>在这两个优秀的开源项目的支持下，我们的研究与实验无论是效率上还是可靠性上都有了很大的提高。</p>
          </div>
        )
      }
    ];
    return (
      <div className="tech-wrapper">
        <div className="section-container">
          {
            TechData.map((item, idx) =>
              <section key={idx}>
                <h1>{item.headline}</h1>
                {item.content}
              </section>)
          }
        </div>
      </div>
    );
  }
}
